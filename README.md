# Simulation-Based Data Generation for Machine Learning

Discrete-Event Simulation + Multi-Model Regression Analysis

**Author:** Anshul Kaushal

---

## 1. Project Overview

This project demonstrates how simulation-based modeling can be used to generate synthetic datasets for machine learning.

A discrete-event simulation of a bank queue system was implemented using SimPy, and 1000 simulation runs were generated by randomly varying system parameters. The generated dataset was then used to train and compare 12 machine learning regression models.

**Objective:** Predict the average customer waiting time based on system parameters

## 2. Simulation Model Description

The system simulates:

- Random customer arrivals
- Exponential service times
- Multiple service counters
- Queue formation when servers are busy

### Parameters Used

| Parameter | Range |
|-----------|-------|
| arrival_rate | 1 â€“ 20 |
| service_rate | 1 â€“ 15 |
| num_servers | 1 â€“ 5 |
| simulation_time | 100 â€“ 500 |

## 3. Data Generation

Random parameter sets were generated using uniform distributions. Each set was passed to the simulation, and the average waiting time was recorded.

**Total simulations performed:** 1000

### Sample Data

| arrival_rate | service_rate | num_servers | simulation_time | avg_wait_time |
|--------------|--------------|-------------|-----------------|---------------|
| 5.94 | 7.74 | 5 | 408 | 0.000025 |
| 9.55 | 10.09 | 2 | 443 | 0.029708 |
| 17.62 | 3.88 | 3 | 336 | 61.983749 |

This shows high variability and nonlinear behavior in the system.

## 4. Cross-Validation Strategy

- **5-Fold Cross Validation** was used
- Mean RÂ² score across folds was recorded
- Prevents overfitting and ensures robustness

## 5. Machine Learning Models Compared (12 Models)

### Linear Models

- Linear Regression
- Ridge
- Lasso
- ElasticNet

### Tree-Based Models

- Decision Tree
- Random Forest
- Extra Trees
- Gradient Boosting

### Boosting / Advanced

- AdaBoost
- XGBoost

### Other

- KNN
- MLP Regressor

## 6. Model Comparison Results

### Performance Table (Sorted by Test RÂ²)

| Model | CV Mean RÂ² | Test RÂ² | RMSE | MAE | Training Time (sec) |
|-------|-----------|---------|------|-----|-------------------|
| ðŸ¥‡ Extra Trees | 0.9634 | 0.9656 | 6.97 | 3.28 | 3.21 |
| ðŸ¥ˆ XGBoost | 0.9354 | 0.9439 | 8.90 | 4.51 | 5.49 |
| ðŸ¥‰ Random Forest | 0.9286 | 0.9222 | 10.49 | 4.97 | 7.23 |
| Gradient Boosting | 0.8882 | 0.8912 | 12.40 | 7.55 | 1.95 |
| MLP Regressor | 0.8716 | 0.8887 | 12.54 | 7.23 | 14.17 |
| Decision Tree | 0.8542 | 0.8097 | 16.40 | 7.24 | 0.07 |
| AdaBoost | 0.6872 | 0.7079 | 20.32 | 17.76 | 1.75 |
| Linear Regression | 0.4408 | 0.4886 | 26.89 | 19.75 | 0.06 |
| Ridge | 0.4408 | 0.4886 | 26.89 | 19.75 | 0.03 |
| Lasso | 0.4407 | 0.4880 | 26.90 | 19.61 | 0.04 |
| ElasticNet | 0.4352 | 0.4816 | 27.07 | 19.43 | 0.03 |
| KNN | 0.3299 | 0.4526 | 27.82 | 15.95 | 0.09 |

## 7. Best Model

### ðŸ¥‡ Best Performing Model: Extra Trees Regressor

- **Highest Test RÂ²:** 0.9656
- **Lowest RMSE:** 6.97
- **Lowest MAE** among top models
- **Very strong cross-validation score:** 0.9634

This indicates excellent generalization performance.

## 8. Hyperparameter Tuning (Random Forest)

GridSearchCV was used for Random Forest optimization.

### Best Parameters Found

```json
{
  "max_depth": 10,
  "min_samples_split": 2,
  "n_estimators": 100
}
```

### Tuned Performance

- RÂ² = 0.9218
- RMSE = 10.52

Although tuning improved performance, Extra Trees still outperformed it.

## 9. Feature Importance Analysis

Using Random Forest:

| Feature | Importance |
|---------|-----------|
| service_rate | 0.3964 |
| arrival_rate | 0.2329 |
| num_servers | 0.1935 |
| simulation_time | 0.1772 |

### Key Insight

- Service rate is the most influential factor
- Higher service rate significantly reduces waiting time
- Arrival rate also strongly impacts congestion
- Number of servers plays an important structural role
- Simulation time has comparatively smaller effect

These findings align with queueing theory principles.

## 10. Why Tree-Based Models Performed Better

The queue system exhibits:

- Nonlinear relationships
- Interaction effects
- Threshold-based behavior
- High variance in outputs

Tree ensemble models like Extra Trees and XGBoost:

âœ” Capture nonlinear relationships  
âœ” Handle feature interactions  
âœ” Reduce variance through averaging  
âœ” Perform implicit feature selection  

Linear models assume linear relationships and therefore underperform.

## 11. Key Conclusions

âœ” Successfully built discrete-event simulation model  
âœ” Generated 1000 synthetic data samples  
âœ” Evaluated 12 ML models  
âœ” Applied 5-Fold cross-validation  
âœ” Performed hyperparameter tuning  
âœ” Conducted feature importance analysis  

### Final Outcome

Extra Trees Regressor achieved the best predictive performance with **RÂ² = 0.9656**, demonstrating the effectiveness of ensemble tree-based methods for nonlinear simulation-generated datasets.

## 13. Result Visualizations

### 1. Correlation Heatmap
<img width="733" height="624" alt="download" src="https://github.com/user-attachments/assets/2f908bb2-f466-4c02-9ffb-fe4d329feb7b" />

The correlation heatmap illustrates the relationships between simulation parameters and the target variable (average waiting time).

#### Key Observations

- **Arrival rate** shows a positive correlation (0.31) with waiting time, meaning higher arrival rates increase congestion
- **Service rate** shows a strong negative correlation (-0.45), indicating that faster service significantly reduces waiting time
- **Number of servers** negatively correlates (-0.36) with waiting time, confirming that increasing service counters reduces queue delay
- **Simulation time** has a weaker positive correlation (0.18) with waiting time

These findings align with theoretical queueing system behavior.

### 2. Model Comparison (RÂ² Score)
<img width="846" height="639" alt="download" src="https://github.com/user-attachments/assets/868e5b53-9271-4c1d-b173-4105395af435" />


The model comparison graph evaluates 12 machine learning models based on their Test RÂ² scores.

#### Performance Ranking

1. **Extra Trees Regressor** â€“ RÂ² = 0.9656
2. **XGBoost** â€“ RÂ² = 0.9439
3. **Random Forest** â€“ RÂ² = 0.9222
4. **Gradient Boosting** â€“ RÂ² = 0.8912
5. **MLP Regressor** â€“ RÂ² = 0.8887

Linear models (Linear, Ridge, Lasso, ElasticNet) performed significantly lower with RÂ² around 0.48, indicating that the system behavior is nonlinear. Tree-based ensemble models clearly outperform linear approaches due to their ability to capture feature interactions and nonlinear dynamics.

### 3. Feature Importance Analysis (Random Forest)
<img width="680" height="525" alt="download" src="https://github.com/user-attachments/assets/bb1aafa1-edde-433b-ab7f-53f20f71df1f" />
Feature importance analysis reveals the relative influence of each parameter on average waiting time.

#### Importance Ranking

1. **Service Rate** â€“ 39.64%
2. **Arrival Rate** â€“ 23.29%
3. **Number of Servers** â€“ 19.35%
4. **Simulation Time** â€“ 17.72%

The most influential factor is service rate, confirming that improving service efficiency has the greatest impact on reducing waiting time. Arrival rate and number of servers also significantly affect system performance.

### 4. Actual vs Predicted (Best Model â€“ Extra Trees)
<img width="571" height="455" alt="download" src="https://github.com/user-attachments/assets/116110a2-217c-4cdc-b75a-9535ebdcaf17" />

The Actual vs Predicted scatter plot demonstrates the predictive performance of the best-performing model (Extra Trees Regressor).

#### Observations

- Data points closely follow a diagonal trend, indicating strong agreement between predicted and actual values
- Minimal dispersion around the trend line reflects low prediction error
- The model effectively captures both low and high waiting time values, including extreme congestion cases

This visual confirmation supports the high RÂ² score (0.9656) obtained by the model.

## 14. Future Improvements

- Add SHAP explainability
- Perform sensitivity analysis
- Increase simulations to 5000+
- Compare LightGBM and CatBoost
- Deploy as a web app

---

## Final Statement

This project demonstrates how modeling and simulation can effectively generate structured synthetic datasets, which can then be leveraged for machine learning experimentation and predictive modeling in complex systems.

**ðŸš€ Explore the power of simulation-based machine learning!**
